import csv
from langchain.schema import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA

# Step 1: CSV ржерзЗржХрзЗ ржбрж╛ржЯрж╛ ржкржбрж╝рзЗ ржПржХржЯрж╛ржирж╛ ржЯрзЗржХрзНрж╕ржЯ ржмрж╛ржирж╛ржирзЛ
def read_csv_as_text(file_path):
    with open(file_path, encoding='utf-8') as f:
        reader = csv.DictReader(f)
        text = ""
        for row in reader:
            # ржЙржжрж╛рж╣рж░ржг рж╣рж┐рж╕рзЗржмрзЗ leave_policy.csv ржПрж░ ржЬржирзНржп
            text += f"Employee {row['EmployeeName']} ({row['EmployeeID']}) is allowed {row['AllowedDays']} days {row['LeaveType']} in {row['Year']}, taken {row['TakenDays']} days.\n"
    return text

# CSV ржлрж╛ржЗрж▓рзЗрж░ ржирж╛ржо ржПржЦрж╛ржирзЗ рж╕рзЗржЯ ржХрж░рзЛ
csv_file = "leave_policy.csv"

# Step 2: ржбрж╛ржЯрж╛ рж▓рзЛржб ржУ Document ржП рж░рзВржкрж╛ржирзНрждрж░
data_text = read_csv_as_text(csv_file)
documents = [Document(page_content=data_text)]

# Step 3: ржЯрзЗржХрзНрж╕ржЯ рж╕рзНржкрзНрж▓рж┐ржЯ ржХрж░рж╛ (chunk ржмрж╛ржирж╛ржирзЛ)
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
texts = text_splitter.split_documents(documents)

# Step 4: ржПржорзНржмрзЗржбрж┐ржВ рждрзИрж░рж┐
embeddings = OllamaEmbeddings(model="nomic-embed-text")
db = FAISS.from_documents(texts, embeddings)

# Step 5: LLM ржУ QA Chain рждрзИрж░рж┐
llm = OllamaLLM(model="mistral")
qa = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())

# Step 6: ржЗржЙржЬрж╛рж░ ржЗржирзНржЯрж╛рж░ржлрзЗрж╕ (ржЗржиржкрзБржЯ ржЖржЙржЯржкрзБржЯ рж▓рзБржк)
print("\nЁЯдЦ Chatbot ржЪрж╛рж▓рзБ тЬЕ (exit рж▓рж┐ржЦрзЗ ржмрзЗрж░ рж╣рзЛржи)\n")
while True:
    query = input("ЁЯзС ржЖржкржирж┐: ")
    if query.lower() == "exit":
        print("ржмрж┐ржжрж╛рзЯ! ЁЯШК")
        break
    answer = qa.invoke({"query": query})
    print("ЁЯдЦ Bot:", answer["result"], "\n")
