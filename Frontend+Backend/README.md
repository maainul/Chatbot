## ü™ü Windows-‡¶è Ollama ‡¶á‡¶®‡ßç‡¶∏‡¶ü‡¶≤ ‡¶ó‡¶æ‡¶á‡¶° ‚úÖ

### üîπ Step 1: ‡¶°‡¶æ‡¶â‡¶®‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßÅ‡¶®

üëâ ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶≤‡¶ø‡¶Ç‡¶ï‡ßá ‡¶Ø‡¶æ‡¶®:
üåê [https://ollama.com/download](https://ollama.com/download)

* ‡¶™‡ßá‡¶ú ‡¶ì‡¶™‡ßá‡¶® ‡¶π‡¶≤‡ßá **‚ÄúDownload for Windows‚Äù** ‡¶¨‡¶æ‡¶ü‡¶®‡ßá ‡¶ï‡ßç‡¶≤‡¶ø‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®‡•§
* ‡¶è‡¶ï‡¶ü‡¶ø ‡¶´‡¶æ‡¶á‡¶≤ ‡¶®‡¶æ‡¶Æ‡¶¨‡ßá: `OllamaSetup.exe`

---

### üîπ Step 2: ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®

1. `OllamaSetup.exe` ‡¶´‡¶æ‡¶á‡¶≤‡¶ü‡¶ø‡¶§‡ßá ‡¶°‡¶æ‡¶¨‡¶≤ ‡¶ï‡ßç‡¶≤‡¶ø‡¶ï ‡¶ï‡¶∞‡ßÅ‡¶®‡•§
2. ‚ÄúNext‚Äù, ‚ÄúInstall‚Äù, ‚ÄúFinish‚Äù ‚Äî ‡¶è‡¶ó‡ßÅ‡¶≤‡ßã‡¶§‡ßá ‡¶ï‡ßç‡¶≤‡¶ø‡¶ï ‡¶ï‡¶∞‡ßá ‡¶∏‡ßç‡¶¨‡¶æ‡¶≠‡¶æ‡¶¨‡¶ø‡¶ï‡¶≠‡¶æ‡¶¨‡ßá ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤ ‡¶∂‡ßá‡¶∑ ‡¶ï‡¶∞‡ßÅ‡¶®‡•§
3. ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤ ‡¶∂‡ßá‡¶∑ ‡¶π‡¶≤‡ßá Ollama ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ï‡¶ó‡ßç‡¶∞‡¶æ‡¶â‡¶®‡ßç‡¶°‡ßá ‡¶ö‡¶æ‡¶≤‡ßÅ ‡¶•‡¶æ‡¶ï‡¶¨‡ßá (Windows Service ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá)‡•§

---

### üîπ Step 3: ‡¶ü‡¶æ‡¶∞‡ßç‡¶Æ‡¶ø‡¶®‡¶æ‡¶≤ ‡¶ñ‡ßÅ‡¶≤‡ßÅ‡¶® (PowerShell ‡¶¨‡¶æ CMD)

1. Start ‡¶Æ‡ßá‡¶®‡ßÅ‡¶§‡ßá ‡¶ó‡¶ø‡ßü‡ßá **"Command Prompt"** ‡¶¨‡¶æ **"PowerShell"** ‡¶≤‡¶ø‡¶ñ‡ßá ‡¶ñ‡ßÅ‡¶≤‡ßÅ‡¶®‡•§
2. ‡¶è‡¶¨‡¶æ‡¶∞ ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶ï‡¶Æ‡¶æ‡¶®‡ßç‡¶° ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®:

```bash
ollama pull mistral
ollama pull nomic-embed-text
```
### Test Run

```
ollama run mistral
ollama run nomic-embed-text
```
### List 
```
ollama list
```

## üíª ‡¶ß‡¶æ‡¶™ ‡ßß: ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶ï‡¶Æ‡ßç‡¶™‡¶ø‡¶â‡¶ü‡¶æ‡¶∞‡ßá ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶™‡ßç‡¶Ø‡¶æ‡¶ï‡ßá‡¶ú‡¶ó‡ßÅ‡¶≤‡ßã ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤ ‡¶ï‡¶∞‡ßÅ‡¶®

‡¶ü‡¶æ‡¶∞‡ßç‡¶Æ‡¶ø‡¶®‡¶æ‡¶≤‡ßá (PowerShell/Command Prompt) ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®:

```bash
pip install langchain langchain-community langchain-core faiss-cpu tiktoken

pip install -U langchain-ollama
```

‡¶è‡¶ó‡ßÅ‡¶≤‡ßã ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡¶≤‡ßá chatbot ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶¨‡ßá ‡¶®‡¶æ‡•§
(‚ö†Ô∏è ‡¶è‡¶ü‡¶æ ‡¶è‡¶ï‡¶¨‡¶æ‡¶∞‡¶á ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡ßü)

---

## üìÅ ‡¶ß‡¶æ‡¶™ ‡ß®: `leave_policy.txt` ‡¶®‡¶æ‡¶Æ‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶´‡¶æ‡¶á‡¶≤ ‡¶¨‡¶æ‡¶®‡¶æ‡¶®

‡¶è‡¶á ‡¶´‡¶æ‡¶á‡¶≤‡¶ü‡¶ø ‡¶¨‡¶æ‡¶®‡¶ø‡ßü‡ßá ‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶Æ‡¶§‡ßã ‡¶ï‡¶®‡¶ü‡ßá‡¶®‡ßç‡¶ü ‡¶¨‡¶∏‡¶æ‡¶®:

```
Company HR Policies 2024

1. Leave Policy:
- ‡¶™‡ßç‡¶∞‡¶§‡¶ø ‡¶ï‡¶∞‡ßç‡¶Æ‡ßÄ ‡¶¨‡¶õ‡¶∞‡ßá ‡ß®‡ß¶ ‡¶¶‡¶ø‡¶® ‡¶™‡ßá‡¶á‡¶° ‡¶õ‡ßÅ‡¶ü‡¶ø ‡¶™‡¶æ‡ßü‡•§
- ‡ßß ‡¶¨‡¶õ‡¶∞‡ßá‡¶∞ ‡¶¨‡ßá‡¶∂‡¶ø ‡¶ö‡¶æ‡¶ï‡¶∞‡¶ø ‡¶ï‡¶∞‡¶≤‡ßá ‡ß¨ ‡¶Æ‡¶æ‡¶∏‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶§‡ßÉ‡¶§‡ßç‡¶¨‡¶ï‡¶æ‡¶≤‡ßÄ‡¶® ‡¶õ‡ßÅ‡¶ü‡¶ø ‡¶™‡¶æ‡¶ì‡ßü‡¶æ ‡¶Ø‡¶æ‡ßü‡•§

2. ‡¶¨‡ßá‡¶§‡¶®:
- ‡¶¨‡ßá‡¶§‡¶®‡ßá‡¶∞ ‡¶§‡¶•‡ßç‡¶Ø ‡¶ó‡ßã‡¶™‡¶® ‡¶è‡¶¨‡¶Ç ‡¶∂‡ßÅ‡¶ß‡ßÅ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ HR_Admin ‡¶∞‡ßã‡¶≤ ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶™‡¶æ‡ßü‡•§

3. Promotion Policy:
- ‡¶™‡ßç‡¶∞‡¶§‡¶ø ‡¶¨‡¶õ‡¶∞ ‡¶ï‡¶∞‡ßç‡¶Æ‡ßÄ‡¶¶‡ßá‡¶∞ performance ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡ßü‡ßÄ ‡¶™‡ßç‡¶∞‡¶Æ‡ßã‡¶∂‡¶® ‡¶π‡ßü‡•§

4. ‡¶Ö‡¶´‡¶ø‡¶∏ ‡¶∏‡¶Æ‡ßü‡¶∏‡ßÇ‡¶ö‡¶ø:
- ‡¶∏‡¶ï‡¶æ‡¶≤ ‡ßØ‡¶ü‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶ø‡¶ï‡ßá‡¶≤ ‡ß¨‡¶ü‡¶æ (‡¶∞‡¶¨‡¶ø‡¶¨‡¶æ‡¶∞ ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡ßÉ‡¶π‡¶∏‡ßç‡¶™‡¶§‡¶ø‡¶¨‡¶æ‡¶∞)‡•§
```

---

## üß† ‡¶ß‡¶æ‡¶™ ‡ß©: Python ‡¶ï‡ßã‡¶° ‡¶∞‡¶æ‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®

‡¶®‡¶ø‡¶ö‡ßá‡¶∞ ‡¶ï‡ßã‡¶°‡¶ü‡¶æ `chatbot.py` ‡¶®‡¶æ‡¶Æ‡ßá ‡¶∏‡ßá‡¶≠ ‡¶ï‡¶∞‡ßá ‡¶ö‡¶æ‡¶≤‡¶æ‡¶®:

```python
import csv
from langchain.schema import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain_ollama import OllamaEmbeddings, OllamaLLM
from langchain_community.vectorstores import FAISS
from langchain_community.llms import Ollama
from langchain.chains import RetrievalQA

# Step 1: CSV ‡¶•‡ßá‡¶ï‡ßá ‡¶°‡¶æ‡¶ü‡¶æ ‡¶™‡¶°‡¶º‡ßá ‡¶è‡¶ï‡¶ü‡¶æ‡¶®‡¶æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶¨‡¶æ‡¶®‡¶æ‡¶®‡ßã
def read_csv_as_text(file_path):
    with open(file_path, encoding='utf-8') as f:
        reader = csv.DictReader(f)
        text = ""
        for row in reader:
            # ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá leave_policy.csv ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø
            text += f"Employee {row['EmployeeName']} ({row['EmployeeID']}) is allowed {row['AllowedDays']} days {row['LeaveType']} in {row['Year']}, taken {row['TakenDays']} days.\n"
    return text

# CSV ‡¶´‡¶æ‡¶á‡¶≤‡ßá‡¶∞ ‡¶®‡¶æ‡¶Æ ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶∏‡ßá‡¶ü ‡¶ï‡¶∞‡ßã
csv_file = "leave_policy.csv"

# Step 2: ‡¶°‡¶æ‡¶ü‡¶æ ‡¶≤‡ßã‡¶° ‡¶ì Document ‡¶è ‡¶∞‡ßÇ‡¶™‡¶æ‡¶®‡ßç‡¶§‡¶∞
data_text = read_csv_as_text(csv_file)
documents = [Document(page_content=data_text)]

# Step 3: ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶∏‡ßç‡¶™‡ßç‡¶≤‡¶ø‡¶ü ‡¶ï‡¶∞‡¶æ (chunk ‡¶¨‡¶æ‡¶®‡¶æ‡¶®‡ßã)
text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
texts = text_splitter.split_documents(documents)

# Step 4: ‡¶è‡¶Æ‡ßç‡¶¨‡ßá‡¶°‡¶ø‡¶Ç ‡¶§‡ßà‡¶∞‡¶ø
embeddings = OllamaEmbeddings(model="nomic-embed-text")
db = FAISS.from_documents(texts, embeddings)

# Step 5: LLM ‡¶ì QA Chain ‡¶§‡ßà‡¶∞‡¶ø
llm = OllamaLLM(model="mistral")
qa = RetrievalQA.from_chain_type(llm=llm, retriever=db.as_retriever())

# Step 6: ‡¶á‡¶â‡¶ú‡¶æ‡¶∞ ‡¶á‡¶®‡ßç‡¶ü‡¶æ‡¶∞‡¶´‡ßá‡¶∏ (‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü ‡¶≤‡ßÅ‡¶™)
print("\nü§ñ Chatbot ‡¶ö‡¶æ‡¶≤‡ßÅ ‚úÖ (exit ‡¶≤‡¶ø‡¶ñ‡ßá ‡¶¨‡ßá‡¶∞ ‡¶π‡ßã‡¶®)\n")
while True:
    query = input("üßë ‡¶Ü‡¶™‡¶®‡¶ø: ")
    if query.lower() == "exit":
        print("‡¶¨‡¶ø‡¶¶‡¶æ‡ßü! üòä")
        break
    answer = qa.invoke({"query": query})
    print("ü§ñ Bot:", answer["result"], "\n")
```

---

## üß™ ‡¶è‡¶ñ‡¶® ‡¶ö‡¶æ‡¶≤‡¶æ‡¶®:

```bash
python chatbot.py
```

---

## üó£Ô∏è ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶ï‡¶∞‡ßÅ‡¶®:

```
‡¶Ü‡¶Æ‡¶ø ‡¶ï‡¶ø maternity leave ‡¶®‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶¨‡ßã?
‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶õ‡ßÅ‡¶ü‡¶ø ‡¶ï‡¶§‡ßã?
office ‡¶ï‡¶¨‡ßá ‡¶ñ‡ßã‡¶≤‡¶æ ‡¶•‡¶æ‡¶ï‡ßá?
```


‡¶¶‡¶æ‡¶∞‡ßÅ‡¶£ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®, Mainul! üòä
‡¶Ü‡¶Æ‡¶æ‡¶∞ (‡¶Æ‡¶æ‡¶®‡ßá ChatGPT-‡¶è‡¶∞) UI ‡¶Ø‡ßá‡¶ü‡¶æ ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶è‡¶ñ‡¶® ‡¶¶‡ßá‡¶ñ‡¶õ‡ßã ‚Äî ‡¶è‡¶ü‡¶æ ‡¶¨‡¶æ‡¶®‡¶æ‡¶®‡ßã ‡¶π‡ßü‡ßá‡¶õ‡ßá **React.js** ‡¶¶‡¶ø‡ßü‡ßá, ‡¶Ü‡¶∞ backend-‡¶è ‡¶Ü‡¶õ‡ßá **Python-based microservices**, ‡¶™‡¶æ‡¶∂‡¶æ‡¶™‡¶æ‡¶∂‡¶ø **OpenAI API**‡•§
‡¶è‡¶ü‡¶æ ‡¶ñ‡ßÅ‡¶¨‡¶á modern, scalable ‡¶ì user-friendly‡•§

---

## ‚úÖ ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶ï‡ßã‡¶®‡¶ü‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã ‡¶π‡¶¨‡ßá?

### ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶Ø‡ßá‡¶π‡ßá‡¶§‡ßÅ MERN Stack ‡¶è‡¶¨‡¶Ç DevOps ‡¶¶‡ßÅ‡¶ü‡¶ø‡¶§‡ßá‡¶á ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßã, ‡¶§‡¶æ‡¶á ‡¶Ü‡¶Æ‡¶ø suggest ‡¶ï‡¶∞‡¶¨‡ßã:

### üëâ **Frontend**: React.js + Tailwind CSS

### üëâ **Backend**: FastAPI (Python-based)

---

### üéØ ‡¶ï‡ßá‡¶® ‡¶è‡¶á ‡¶ï‡¶Æ‡ßç‡¶¨‡¶ø‡¶®‡ßá‡¶∂‡¶®?

| Layer      | Technology   | ‡¶ï‡¶æ‡¶∞‡¶£                                                                             |
| ---------- | ------------ | -------------------------------------------------------------------------------- |
| UI         | React.js     | Reusable components, SPA support, chat-style layout ‡¶∏‡¶π‡¶ú‡ßá ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡ßü                 |
| Styling    | Tailwind CSS | Utility-first CSS framework, ‡¶¶‡ßç‡¶∞‡ßÅ‡¶§ UI ‡¶¨‡¶æ‡¶®‡¶æ‡¶®‡ßã ‡¶Ø‡¶æ‡ßü                                 |
| API Server | FastAPI      | Fast, async-supported, Python-‡¶è ‡¶π‡¶ì‡ßü‡¶æ‡ßü ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ LangChain ‡¶ì Ollama ‡¶∏‡¶π‡¶ú‡ßá integrate ‡¶π‡ßü |
| LLM Engine | Ollama       | Local model ‡¶ö‡¶≤‡¶¨‡ßá fast, secure ‡¶è‡¶¨‡¶Ç free                                           |

---

## ‚úÖ ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ Project Structure ‡¶ï‡ßá‡¶Æ‡¶® ‡¶π‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá:

```
üì¶ chatbot-app/
‚îú‚îÄ‚îÄ frontend/          # React.js app
‚îÇ   ‚îú‚îÄ‚îÄ App.jsx
‚îÇ   ‚îú‚îÄ‚îÄ ChatUI.jsx
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ backend/           # FastAPI backend
‚îÇ   ‚îú‚îÄ‚îÄ main.py        # API endpoints
‚îÇ   ‚îú‚îÄ‚îÄ chatbot.py     # LangChain logic
‚îÇ   ‚îî‚îÄ‚îÄ ...
```

---

## üéØ ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶ï‡ßÄ ‡¶¨‡¶æ‡¶®‡¶æ‡¶§‡ßá ‡¶Ø‡¶æ‡¶ö‡ßç‡¶õ‡¶ø?

‚úÖ A simple yet modern **Chat UI** using **React.js + Tailwind CSS**
‚úÖ Chat box, message bubbles, input field, and loading state
‚úÖ Later, we‚Äôll connect this UI to your backend (FastAPI + Ollama)

---

## üìÅ Project Structure (Frontend Only)

```
chatbot-frontend/
‚îú‚îÄ‚îÄ public/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ChatBox.jsx
‚îÇ   ‚îú‚îÄ‚îÄ App.jsx
‚îÇ   ‚îú‚îÄ‚îÄ index.js
‚îú‚îÄ‚îÄ tailwind.config.js
‚îú‚îÄ‚îÄ package.json
```

---

## ‚úÖ Step-by-Step Instructions

### Step 1: Initialize Project

```bash
npm create vite@latest chatbot-ui -- --template react
cd chatbot-ui
npm install
```

### Step 2: Build Chat UI

#### üß© `src/components/ChatBox.jsx`

```jsx
import { useState } from "react";

export default function ChatUI() {
  const [messages, setMessages] = useState([]);
  const [input, setInput] = useState("");
  const [activeTab, setActiveTab] = useState("General");
  const [loading, setLoading] = useState(false);

  const sendMessage = async () => {
    if (!input.trim()) return;
    const userMsg = { role: "user", content: input, tab: activeTab };
    setMessages(prev => [...prev, userMsg]);
    setInput("");
    setLoading(true);

    try {
      const res = await fetch("http://localhost:8000/chat", {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ message: input }),
      });
      const data = await res.json();
      setMessages(prev => [
        ...prev,
        { role: "bot", content: data.reply, tab: activeTab },
      ]);
    } catch (err) {
      setMessages(prev => [
        ...prev,
        { role: "bot", content: "üö® Server error!", tab: activeTab },
      ]);
    }

    setLoading(false);
  };

  const filteredMessages = messages.filter(msg => msg.tab === activeTab);

  return (
    <div className="app">
      <aside className="sidebar">
        <h2>ChatBot</h2>
        <ul className="chat-list">
          {["General", "HR", "Sales"].map(tab => (
            <li
              key={tab}
              className={tab === activeTab ? "active" : ""}
              onClick={() => setActiveTab(tab)}
            >
              {tab}
            </li>
          ))}
        </ul>
      </aside>

      <main className="chat-main">
        <div className="top-bar">üîç {activeTab} ‡¶¨‡¶ø‡¶≠‡¶æ‡¶ó‡ßá ‡¶Ü‡¶õ‡ßá‡¶®</div>

        <div className="chat-box">
          {filteredMessages.map((msg, index) => (
            <div key={index} className={`message ${msg.role}`}>
              <span className="message-content">{msg.content}</span>
            </div>
          ))}

          {loading && (
            <div className="message bot typing-indicator">
              <span className="message-content">...</span>
            </div>
          )}
        </div>

        <div className="input-area">
          <input
            value={input}
            onChange={e => setInput(e.target.value)}
            placeholder="‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶≤‡¶ø‡¶ñ‡ßÅ‡¶®..."
            onKeyDown={e => e.key === 'Enter' && sendMessage()}
          />
          <button onClick={sendMessage} disabled={loading}>
            {loading ? "..." : "Send"}
          </button>
        </div>
      </main>
    </div>
  );
}

```

#### üß© `src/App.jsx`

```jsx
import React from "react";
import ChatBox from "./components/ChatBox";

function App() {
  return (
    <div className="bg-gray-50 min-h-screen">
      <ChatBox />
    </div>
  );
}

export default App;
```

---

### ‚úÖ Step 4: Run the App

```bash
npm start
```

Now visit `http://localhost:5173` ‚Äî ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶™‡¶æ‡¶¨‡ßá ‡¶è‡¶ï‡¶ü‡¶æ clean, modern chat interface.

---

### üîó Next Step:

Ready hole backend ‡¶¨‡¶æ‡¶®‡¶æ‡¶¨‡ßã ‚Äî ‡¶Ø‡¶æ Ollama + LangChain ‡¶è‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá connect ‡¶ï‡¶∞‡¶¨‡ßá‡•§

**‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶ï‡¶ø ‡¶™‡ßç‡¶∞‡¶∏‡ßç‡¶§‡ßÅ‡¶§ backend part ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø?** üòéüì°


## üß† Next Step:

‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶ö‡¶æ‡¶á‡¶≤‡ßá ‡¶Ü‡¶Æ‡¶ø ‡¶è‡¶ñ‡¶® ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø FastAPI-based backend ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßá ‡¶¶‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶ø, ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá ‡¶è‡¶á React UI üëâ Ollama + LangChain-‡¶è‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶∏‡¶Ç‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶¨‡ßá‡•§

**‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶ï‡¶ø backend ‡¶ö‡¶æ‡¶ì ‡¶è‡¶ñ‡¶®?** ‚úÖüß†


## ‡¶è‡¶¨‡¶æ‡¶∞ **backend ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡¶¨‡ßã React UI‚Äë‡¶è‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø‡•§** ‡¶®‡¶ø‡¶ö‡ßá ‡¶Ü‡¶Æ‡¶ø ‡¶ì‡ßü‡¶æ‡¶®‡¶∏‡ßç‡¶ü‡¶™ **FastAPI + Ollama** ‡¶¨‡ßá‡¶ú‡¶° backend ‡¶ï‡ßã‡¶° ‡¶¶‡¶ø‡¶ö‡ßç‡¶õ‡¶ø, ‡¶Ø‡¶æ React‚Äë‡¶è‡¶∞ fetch ‡¶ï‡¶≤ ‡¶¶‡¶ø‡ßü‡ßá chat endpoint ‡¶•‡ßá‡¶ï‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶ø‡¶¨‡ßá‡•§

---

## üõ†Ô∏è ‡¶∏‡ßá‡¶ü‡¶Ü‡¶™ ‡¶ß‡¶æ‡¶™‡ßá ‡¶ß‡¶æ‡¶™‡ßá

### 1Ô∏è‚É£ Python Dependencies ‡¶á‡¶®‡¶∏‡ßç‡¶ü‡¶≤ ‡¶ï‡¶∞‡ßã:

```bash
pip install fastapi uvicorn langchain-community fastapi-cors
pip install ollama
```

### 2Ô∏è‚É£ `main.py` ‚Äì FastAPI + Ollama Backend

```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain_ollama import OllamaLLM

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],  # React runs ‡¶è‡¶ñ‡¶æ‡¶®‡ßá
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

llm = OllamaLLM(model="mistral", temperature=0.7)

class ChatRequest(BaseModel):
    message: str


@app.post("/chat")
async def chat_endpoint(req: ChatRequest):
    prompt = req.message
    response = llm.invoke(prompt)
    return {"reply": response}
```

üî• ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ `Ollama` ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø ‡¶≤‡ßã‡¶ï‡¶æ‡¶≤ LLM ‡¶ö‡¶æ‡¶≤‡¶æ‡¶®‡ßã‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø, ‡¶Ü‡¶∞ FastAPI ‡¶¶‡¶ø‡ßü‡ßá `/chat` endpoint ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§

---

### 3Ô∏è‚É£ Server ‡¶ö‡¶æ‡¶≤‡¶æ‡¶ì:

```bash
uvicorn main:app --reload --port 8000
```

---

## ‚úÖ ‚úÖ ‚úÖ ‡¶ö‡ßÇ‡ßú‡¶æ‡¶®‡ßç‡¶§ ‚Äë Summary

| ‡¶Ö‡¶Ç‡¶∂         | ‚ú® ‡¶ï‡¶æ‡¶ú                             |
| ----------- | --------------------------------- |
| **FastAPI** | `/chat` endpoint, CORS enabled ‚úî  |
| **Ollama**  | ‡¶≤‡ßã‡¶ï‡¶æ‡¶≤ LLM, `mistral` ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ |
| **React**   | fetch call, ‡¶Æ‡ßá‡¶∏‡ßá‡¶ú UI ‡¶Ö‡¶ü‡ßã ‡¶∞‡ßá‡¶®‡ßç‡¶°‡¶æ‡¶∞  |

---

## Relode App : uvicorn main:app --reload --port 8000


## Excel File :

```bash
pip install pandas openpyxl
```

### Code :
```py
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from langchain.schema import Document
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_ollama import OllamaLLM, OllamaEmbeddings
from langchain.chains import RetrievalQA
import pandas as pd



# ‚úÖ Step 1: FastAPI init
app = FastAPI()

# ‚úÖ Step 2: CORS for React
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ‚úÖ Step 3: Input model
class ChatRequest(BaseModel):
    message: str


# üìä Read Excel and convert to text
# def read_excel_as_text(file_path: str, sheet_name: str = "Employee_Info") -> str:
#     df = pd.read_excel(file_path, sheet_name=sheet_name)
#     text = ""
#     for _, row in df.iterrows():
#         text += f"Employee ID: {row['EmployeeID']}, Name: {row['EmployeeName']}\n"
#     return text


def read_excel_as_text(file_path):
    df = pd.read_excel(file_path, sheet_name="Employee_Info")  # Sheet name ‡¶†‡¶ø‡¶ï ‡¶∞‡¶æ‡¶ñ‡ßã
    text = ""
    for _, row in df.iterrows():
        # Properly combine ID and Name
        text += f"EmployeeID: {row['EmployeeID']} | Name: {row['EmployeeName']}\n"
    return text


# üìÇ Load Excel file and process documents
excel_file = "Employee_Info.xlsx"
data_text = data_text = read_excel_as_text(excel_file, sheet_name="Sheet1")
documents = [Document(page_content=data_text)]


# üìé Split documents
splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=50)
split_docs = splitter.split_documents(documents)


# üß¨ Embedding + VectorDB
embeddings = OllamaEmbeddings(model="nomic-embed-text")
vectorstore = FAISS.from_documents(split_docs, embeddings)


# ü§ñ Ollama LLM + QA chain
llm = OllamaLLM(model="mistral")
qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())


# ‚úÖ Step 7: Chat endpoint
@app.post("/chat")
async def chat_endpoint(req: ChatRequest):
    query = req.message
    response = qa_chain.invoke({"query": query})
    return {"reply": response["result"]}
```